First Passage Percolation: A Comprehensive Overview1. Introduction to First Passage Percolation (FPP)First passage percolation (FPP) constitutes a model of a random metric, typically defined on an infinite graph, which introduces stochasticity into the notion of distance within the graph structure 1. This is achieved by assigning independent and identically distributed (i.i.d.) non-negative lengths, often referred to as passage times, to each edge of the graph 1. The most common underlying structure for this model is the d-dimensional Euclidean lattice, denoted as Z<sup>d</sup>, where edges connect vertices that are nearest neighbors 2. The non-negativity constraint on the passage times ensures that traversal always progresses forward in time or remains stationary, while the i.i.d. assumption simplifies the initial mathematical analysis of the model.For any given path within the graph, its passage time is quantified as the sum of the passage times of all the individual edges that constitute the path 3. This summation aggregates the inherent randomness associated with each edge along the entirety of the path. Subsequently, the first passage time between any two vertices, say x and y, is formally defined as the infimum (or minimum, in the context of finite graphs) of the passage times calculated over all possible paths that connect x and y 3. This value represents the random "shortest distance" between the two points within the defined random metric space. The function that assigns this first passage time between any pair of vertices effectively induces a random pseudo-metric on the graph 3. This pseudo-metric satisfies the properties of non-negativity and the triangle inequality. If the edge weights are strictly positive, then this becomes a true random metric.The inherent simplicity of the FPP model, characterized by its definition on a lattice structure with i.i.d. weights, is noteworthy as it allows for the formulation of numerous intriguing conjectures without necessitating overly complex definitions 3. This balance between a straightforward setup and the emergence of complex behaviors is a central aspect that makes FPP a compelling model for investigating fundamental questions in probability theory. Furthermore, the focus on the infimum over all possible paths underscores the intrinsic connection of FPP to optimization problems within a stochastic environment 3. This perspective links the study of FPP to broader areas such as network optimization and the analysis of shortest path algorithms.1.2 Background and HistoryThe formal introduction of first passage percolation as a mathematical model occurred in 1965, attributed to the seminal work of John Hammersley and Dominic Welsh 2. Their initial motivation stemmed from the practical problem of mathematically describing the flow of fluids through inhomogeneous or porous materials 2. In this context, the random weights assigned to the edges of the lattice can be interpreted as representing the varying degrees of permeability within the medium.Early research efforts in the field primarily concentrated on the model defined on the two-dimensional square lattice, Z<sup>2</sup>, largely due to its relative mathematical simplicity and tractability 2. However, the fundamental framework of FPP has since been extended and applied to a wider variety of more general graph structures. FPP is now recognized as a cornerstone within probability theory, holding a significant place as one of its most classical and fundamental areas, with close ties to the broader field of percolation theory 3. In fact, classical Bernoulli percolation, where edges are either open or closed with certain probabilities, can be considered as a specific instance of FPP where the edge weights take only the values 0 or 1.A pivotal question that was raised in the original 1969 paper by Hammersley and Welsh concerned the long-term behavior of the set of points reachable from the origin within a given time t, formally denoted as B(t) = {y ∈ Z<sup>2</sup> : T(0,y) ≤ t}, as t approaches infinity 3. This question has served as a major driving force behind much of the subsequent research conducted in the field of FPP. This initial inquiry eventually led to the groundbreaking Cox-Durrett limit shape theorem in 1981, a result that significantly advanced the understanding of the large-scale geometric properties of FPP 3.Interestingly, the investigation of FPP has not only yielded results specific to the model itself but has also contributed to the development of important mathematical tools. Most notably, the Subadditive Ergodic Theorem, a fundamental result in ergodic theory, emerged partially from the study of FPP 3. This highlights the broader impact of FPP beyond its specific applications and demonstrates its role in the advancement of mathematical theory. The connection between a concrete physical problem and the development of an abstract mathematical model illustrates how practical considerations can inspire theoretical advancements 2. The fact that a question posed in the 1960s led to a major theorem in the 1980s, and that many questions remain unanswered, underscores the enduring challenges and the ongoing nature of research in FPP 3. This historical perspective emphasizes the depth and complexity inherent in this seemingly simple model.1.3 Related PhenomenaFirst passage percolation exhibits connections and similarities to several other models in probability and statistical physics. Understanding these relationships provides a broader context for the study of FPP and can offer insights into its behavior.Directed Polymers in Random Media: First passage percolation can be interpreted as the zero-temperature limit of the model of directed polymers in a random environment (DPRE) 8. In the DPRE model, paths traversing a random medium are assigned energies based on the properties of the medium. The model then studies the statistical mechanics of these paths at a positive temperature. As the temperature is reduced towards zero, the system becomes increasingly dominated by the path with the minimal energy, which directly corresponds to the geodesic in FPP. Both FPP and DPRE are concerned with the study of paths within a random environment, with FPP specifically focusing on the optimal path (the one with the shortest passage time) and DPRE considering the ensemble of all possible paths, weighted according to a Boltzmann distribution based on their energy 8. Notably, the Kardar-Parisi-Zhang (KPZ) universality class is conjectured to govern the scaling behavior of FPP in two dimensions as well as certain models of directed polymers, suggesting a fundamental underlying connection in their critical phenomena 2. This relationship between FPP and directed polymers bridges the gap between probability theory and statistical mechanics, potentially allowing for the transfer of analytical techniques and insights between the two fields 8. This highlights the unifying power of mathematical concepts across different scientific disciplines.Last Passage Percolation (LPP): Another closely related model is last passage percolation (LPP), where the objective is to find a path that maximizes the sum of the weights along its edges or vertices, in contrast to the minimization objective in FPP 11. Typically, LPP is studied on oriented lattices where random weights are assigned to the vertices. While FPP focuses on identifying the fastest paths (minimum passage times), LPP is concerned with finding the slowest paths (maximum passage times) within a random environment 3. An interesting distinction between the two models lies in their solvability. Several variants of LPP possess the property of being exactly solvable, meaning that many quantities of interest can be calculated explicitly 11. This is in contrast to FPP, where exact solutions are relatively rare. The solvability of LPP can sometimes provide valuable insights or suggest conjectures that might be relevant to the study of FPP. Both FPP and LPP exhibit the phenomenon of a limit shape, which describes the asymptotic growth of the region reachable within a certain time or number of steps 13. Comparing FPP with LPP underscores the significant impact of the optimization direction (minimization versus maximization) on the model's fundamental properties and mathematical tractability 11. The relative ease with which LPP can be solved provides a useful point of comparison for understanding the specific challenges inherent in the analysis of shortest paths in random media.Other Connections: Beyond directed polymers and last passage percolation, FPP shares connections with other phenomena. The Eden growth model, which is used to simulate the growth of clusters such as bacterial colonies, exhibits similarities to the expanding shape observed in FPP 3. The time it takes for the "growth" or "infection" to reach a certain distance in the Eden model can be related to the concept of passage times in FPP. Furthermore, the problem of finding a minimum cost path in FPP has analogies to the Vickrey-Clarke-Groves (VCG) auction mechanism in economics 3. In VCG auctions, the goal is to determine payments based on the shortest paths in a network where costs are associated with the edges. This connection illustrates the broad applicability of shortest path concepts across different disciplines. The concept of ground states in disordered ferromagnets, particularly spin glasses, has links to the existence of infinite geodesics in FPP 12. The uniqueness of a ground state in spin glasses is analogous to the question of whether multiple infinite geodesics can exist in FPP. Additionally, the "slow bond problem" for the Totally Asymmetric Simple Exclusion Process (TASEP), a model used to study interacting particles, is related to the "columnar defect" problem in inhomogeneous FPP 12. This highlights the connections between FPP and other models within the field of interacting particle systems. The diverse range of connections that FPP has to other models in physics, computer science, and economics emphasizes the fundamental nature of the underlying mathematical principles and the broad applicability of the model 3. This interdisciplinary nature makes FPP a rich and valuable area of study.2. Basic Concepts from Percolation TheoryFirst passage percolation is closely related to the field of percolation theory, and understanding some of the fundamental concepts from percolation provides a valuable framework for studying FPP.2.1 Percolation Threshold: In standard Bernoulli percolation, one considers a graph, often a lattice, where each edge (in bond percolation) or each vertex (in site percolation) is independently designated as "open" with a probability p and "closed" with a probability of 1-p. A key concept in this model is the percolation threshold, denoted by p<sub>c</sub>. This critical value of p marks a transition in the behavior of the system. For values of p below p<sub>c</sub>, the probability of finding an infinitely large connected cluster of open elements originating from a given point is zero. Conversely, for p greater than p<sub>c</sub>, this probability becomes positive. At the critical probability p = p<sub>c</sub>, a phase transition occurs in the system's connectivity properties. While FPP assigns continuous non-negative weights to edges rather than discrete binary states, the concept of a threshold can be considered analogously. For instance, one might investigate the probability of the existence of a path between two distant points with a total passage time that falls below a certain value t. As the value of t increases, the likelihood of such a path existing may exhibit a transition, similar in spirit to the percolation threshold observed in Bernoulli percolation. The notion of a percolation threshold in Bernoulli percolation offers a conceptual basis for understanding how connectivity emerges in random systems, and this has parallels in FPP when considering the existence of "fast" paths that can traverse long distances efficiently 3. This analogy helps in developing intuition about the large-scale behavior of FPP by relating it to a more established and well-understood model.2.2 Bond vs. Site Percolation: A primary distinction within percolation theory is between bond percolation and site percolation. In bond percolation, the randomness is associated with the edges (or bonds) of the graph, determining whether each edge is open or closed. In contrast, site percolation involves randomness associated with the vertices (or sites) of the graph, determining whether each vertex is active or inactive. In the context of FPP on a lattice, the random passage times are typically assigned to the edges of the lattice 1. Therefore, the structure of randomness in FPP on a lattice aligns more closely with that of bond percolation. Recognizing that standard FPP on a lattice corresponds to a bond percolation-like setting is helpful for drawing connections to existing results and analytical techniques from that specific area of percolation theory 3. This classification clarifies the nature of the underlying random structure being studied in the context of FPP.2.3 The Notion of Infinite Clusters: In percolation theory, an infinite cluster refers to a connected component of open elements within the random configuration that extends infinitely in size. The existence of such an infinite cluster is a defining characteristic of the percolating phase, which occurs when the probability p of an element being open exceeds the critical threshold p<sub>c</sub>. In FPP, the analog of an infinite cluster can be conceptualized by considering the asymptotic behavior of the set B(t), which represents all points reachable from the origin within a time t. The shape theorem, a fundamental result in FPP, demonstrates that as t approaches infinity, the rescaled set B(t)/t converges almost surely to a deterministic shape R. The fact that this limit shape R is non-empty implies that it is possible to reach arbitrarily far in any direction within a time that grows linearly with the distance. This behavior is akin to having an "infinite cluster" of points that are reachable within a certain "speed" dictated by the properties of the random medium. The concept of an infinite cluster in percolation provides an intuitive way to understand the implications of the shape theorem in FPP, where the existence of a non-trivial limit shape suggests a form of large-scale connectivity in terms of efficient passage times 3. This connection helps to interpret the abstract result of the shape theorem in a more concrete and intuitive manner.3. Basic Properties of First Passage PercolationFirst passage percolation exhibits several fundamental properties that govern its behavior. These properties are crucial for understanding the model's long-term characteristics and its relationship to other probabilistic systems.3.1 Subadditivity: A key property of the first passage time T(x, y) is its subadditivity. For any three vertices x, y, and z in the graph, the following inequality holds: T(x, z) ≤ T(x, y) + T(y, z) 3. This property arises directly from the definition of T(x, z) as the minimum passage time over all possible paths connecting x and z. Consider any path from x to z that happens to pass through the intermediate vertex y. The total passage time for this specific path is simply the sum of the passage time from x to y and the passage time from y to z. Since T(x, z) is defined as the infimum (minimum) over all possible paths between x and z, its value must necessarily be less than or equal to the passage time of this particular path that goes through y. This holds true for any such intermediate vertex y, hence establishing the subadditive inequality. This property reflects the intuitive notion that introducing an intermediate point along a (near-)optimal path between two endpoints cannot lead to a decrease in the total passage time 4. This seemingly simple property has profound implications for understanding the asymptotic behavior of FPP, as it allows for the application of powerful mathematical tools designed for analyzing subadditive processes.3.2 Law of Large Numbers and the Time Constant: The subadditivity property of the first passage time is crucial because it allows for the application of Kingman's Subadditive Ergodic Theorem 4. This theorem is a fundamental result in ergodic theory that guarantees the existence of a limit for a wide class of subadditive stochastic processes. In the context of FPP, this theorem implies that for any direction vector v in the d-dimensional lattice Z<sup>d</sup>, the expected passage time per unit distance in that direction converges to a deterministic constant known as the time constant, often denoted by μ(v) 4. More formally, this convergence can be expressed as lim<sub>n→∞</sub> T(0, nv)/n = μ(v), and this convergence occurs almost surely (meaning with probability one) and also in the L<sup>1</sup> sense under certain conditions on the moments of the edge weight distribution. The time constant μ(v) represents the asymptotic speed of propagation or the effective cost per unit distance in the direction specified by v. Its specific value is determined by the underlying probability distribution of the random edge weights. This result establishes a form of deterministic behavior in the long run for a system that is inherently random at the local level 4. Despite the stochasticity in the assignment of weights to individual edges, the average passage time observed over sufficiently large distances becomes predictable. Furthermore, the time constant μ(v) can exhibit anisotropy, meaning its value can depend on the direction v 4. This reflects the possibility that the random medium might offer varying degrees of resistance to passage depending on the direction of travel. This directional dependence is closely related to the shape of the limit shape, which will be discussed in more detail later.3.3 Existence of Geodesics: A geodesic between two vertices x and y in the context of FPP is defined as a path that connects these two vertices and whose total passage time is exactly equal to the first passage time T(x, y). In simpler terms, it represents a path of shortest time between the two points. A fundamental question in the study of FPP is whether such geodesics actually exist. That is, whether the infimum in the definition of T(x, y) is attained by a path consisting of a finite number of edges. Under fairly general conditions on the probability distribution of the edge weights, such as the weights being non-negative and the probability of an edge having a weight of exactly zero being sufficiently small, it has been rigorously proven that geodesics between any two points in the lattice Z<sup>d</sup> exist with a probability of one 3. The existence of geodesics ensures that the concept of a "shortest time path" is well-defined within the framework of FPP 3. This allows for the meaningful study of the properties and characteristics of these optimal paths within the random metric space. Without the guarantee of geodesic existence, the analysis would be more abstract, focusing solely on the infimum of passage times, which might not always be achieved by an actual path.4. Kesten's Large Deviation Estimates and the Shape TheoremHarry Kesten made significant and lasting contributions to the field of first passage percolation. Among his many important results are large deviation estimates for the passage time and fundamental insights into the shape theorem.4.1 Kesten's Large Deviation Estimates: Harry Kesten's work included the derivation of important large deviation estimates for the first passage time T(0, nv) 2. These estimates provide quantitative bounds on the probability that the random variable T(0, nv) deviates significantly from its expected value, which, as discussed earlier, is approximately nμ(v) for large values of n. These estimates typically demonstrate that the probability of such large deviations decays exponentially with the distance n. The specific mathematical form of these estimates depends on the particular probability distribution governing the random edge weights. Kesten's large deviation results offer a more refined understanding of the fluctuations of passage times beyond the information provided by the law of large numbers 2. They quantify the likelihood of observing rare events where the actual time taken to travel a large distance is substantially longer or shorter than the average expected time. This type of analysis is crucial for a deeper understanding of the probabilistic behavior of the system around the deterministic limit given by the time constant.4.2 The Shape Theorem: A central and fundamental result in the study of FPP is the Shape Theorem, which was rigorously proven by Cox and Durrett in 1981 3. This theorem provides a precise description of the asymptotic shape of the region that can be reached starting from the origin within a given time t. The theorem states that for any arbitrarily small positive value ε, there exists a random time T (which might depend on the specific realization of the random edge weights) such that for all times t greater than T, the rescaled random set B(t)/t = {x/t : T(0, x) ≤ t} is contained within the deterministic set (1 + ε)R and, conversely, contains the deterministic set (1 − ε)R, with probability one. Here, R represents a non-empty, compact, and convex set in the d-dimensional Euclidean space R<sup>d</sup>, which is also symmetric about the origin. This set R is referred to as the limit shape or the asymptotic shape of the first passage percolation process. The shape of this limit set reflects the anisotropy of the time constant μ(v), which, as noted earlier, can depend on the direction v. In fact, the limit shape R can be characterized as the set of all points x in R<sup>d</sup> for which the extension of the time constant to R<sup>d</sup>, μ(x), is less than or equal to 1. The Shape Theorem is a significant consequence of the Subadditive Ergodic Theorem when applied to the context of first passage percolation 4. This theorem reveals a remarkable regularity in the large-scale behavior of FPP 3. Despite the inherent randomness at the local level of individual edge weights, the overall shape of the region that can be reached by a sufficiently large time t, when appropriately rescaled, converges to a fixed, deterministic shape as t becomes very large. The properties of the limit shape, namely that it is non-empty, compact, convex, and symmetric, provide crucial information about the nature of growth and propagation within the random medium described by FPP 5. For example, the convexity of the limit shape suggests that if two points are reachable within a certain scaled time, then any point lying on the straight line segment connecting them is also reachable within a related scaled time frame.4.3 Conjectures on the Limit Shape: While the Shape Theorem definitively establishes the existence of a limit shape for first passage percolation, many of its detailed geometric properties remain unknown and are the subject of ongoing research. Two prominent conjectures pertain to the regularity of this limit shape:Strict Convexity Conjecture: It is widely believed that under mild conditions on the probability distribution of the edge weights (for instance, if the distribution is not concentrated on a single value), the limit shape R should be strictly convex 3. Strict convexity implies that the boundary of the set R does not contain any straight line segments. If this conjecture holds true, it would suggest that for any given direction of travel, there is a unique "optimal" asymptotic direction to move in to minimize the passage time per unit distance.Uniform Curvature Conjecture: Another significant conjecture proposes that under similar mild conditions on the edge weight distribution, the boundary of the limit shape R should possess uniform positive curvature at all points where it is differentiable 2. This would imply a certain degree of smoothness in the shape of the boundary.These two conjectures are known to be quite challenging to prove in their full generality and remain open problems in most cases. However, there has been progress made in specific scenarios. For example, Auffinger and Damron achieved a significant result by proving the existence of at least one point on the boundary of the limit shape where it is differentiable, specifically in the case where the edge weight distribution allows for the possibility of "flat edges" 3. The open nature of the strict convexity and uniform curvature conjectures highlights the difficulties in fully characterizing the precise geometric nature of the limit shape in FPP 2. These conjectures continue to motivate and guide ongoing research efforts in the field, as their resolution would provide a much deeper and more complete understanding of the geometric properties of the asymptotic shape that emerges in first passage percolation.5. Concentration and Fluctuations of Passage TimesBeyond the average behavior described by the time constant and the asymptotic shape, understanding the concentration and fluctuations of the passage times is crucial for a complete picture of first passage percolation.5.1 Kesten's Variance Bound: As previously mentioned, Harry Kesten's contributions to FPP include not only large deviation estimates but also important results concerning the fluctuations of passage times. Specifically, Kesten proved that the variance of the passage time T(0, nv) is bounded above by a constant multiplied by n: Var(T(0, nv)) ≤ C n 2. Here, C is a constant that depends on the distribution of the edge weights. This linear growth of the variance with distance n indicates that the fluctuations of the passage time around its mean value do not grow faster than linearly with the distance traveled. This result suggests that the passage time, while random, does not exhibit wildly erratic behavior and tends to stay relatively close to its expected value, at least in terms of the rate at which its variance grows with distance. This bound provides a fundamental piece of information about the scale of fluctuations in FPP 2.5.2 Alexander's Method: Alexander's method is a powerful and versatile technique that has found broad applications in the study of disordered systems, including first passage percolation 2. This method is particularly useful for analyzing the fluctuations of certain quantities of interest, often those related to ground state energies or optimal paths in random environments. The general approach of Alexander's method involves introducing a small perturbation to the random environment under consideration, for example, by slightly altering the weight of a single edge in the lattice. The method then focuses on studying how this localized perturbation affects the global quantity of interest, such as the first passage time between two distant points or the structure of the geodesic connecting them. By carefully relating the sensitivity of this quantity to the applied perturbation to the variance of the quantity itself, it becomes possible to derive bounds or gain insights into the nature and magnitude of the fluctuations. In the context of FPP, Alexander's method provides a sophisticated tool for probing the fine-scale structure and the sensitivity of optimal paths and passage times to local changes in the random environment 2. It allows researchers to establish connections between local stochasticity and global fluctuations in the system. This approach highlights the importance of understanding the stability of geodesics and passage times under small perturbations in the underlying random medium.5.3 BKS Inequality: The Benjamini-Kalai-Schramm (BKS) inequality is a general and widely applicable tool within percolation theory that provides an upper bound on the probability of the union of a collection of "local" events. The strength of the BKS inequality lies in its ability to bound this probability in terms of the probabilities of these individual events occurring independently. This inequality has proven to be particularly useful in the study of concentration phenomena in percolation and related models such as first passage percolation 2. For instance, the BKS inequality can be employed to demonstrate that the passage time T(0, nv) is indeed concentrated around its mean value. This means that the probability of observing passage times that deviate significantly from the expected value decays rapidly as the magnitude of the deviation increases. The BKS inequality thus provides a way to transfer information about the probabilities of local events (related to individual edge weights or small regions of the lattice) to global concentration results concerning macroscopic quantities like the total passage time over large distances 2. It offers a method for controlling the likelihood of significant deviations from the typical behavior predicted by the law of large numbers.5.4 Newman-Piza Lower Bound from Aizenman-Wahr Inequality: Newman and Piza made use of the Aizenman-Wahr inequality, which is a result concerning the size of the infinite cluster in standard percolation theory, to derive a lower bound on the variance of the passage time in first passage percolation 2. This lower bound serves as a complement to the upper bound provided by Kesten's result, offering a more complete picture of the order of magnitude of the fluctuations in passage times. Under certain conditions on the distribution of edge weights, the Newman-Piza result suggests that the variance of the passage time typically grows at least linearly with the distance n. This indicates that the fluctuations in passage times are not arbitrarily small and have a growth rate that is at least on the order of n 2. Combining the upper bound from Kesten and the lower bound from Newman and Piza provides a more precise characterization of the typical size of deviations of the passage time from its expected value as a function of the distance traveled.6. Transversal Bounds for GeodesicsUnderstanding the behavior of geodesics, the shortest time paths in FPP, involves not only looking at their overall direction but also quantifying how much they deviate from a straight line connecting their endpoints. These deviations are known as transversal fluctuations.6.1 Results by Newman and Licea: Charles Newman and Cristian Licea have made significant contributions to the understanding of geodesics in first passage percolation, with a particular focus on characterizing their transversal fluctuations 2. Transversal fluctuations refer to the extent to which a geodesic deviates from the direct, straight-line path connecting its starting and ending points, in directions that are perpendicular to the overall direction of travel. Their work provides probabilistic bounds on the magnitude of these deviations. Typically, these bounds demonstrate that the transversal fluctuations grow sublinearly with the distance between the endpoints of the geodesic. For instance, in the two-dimensional case of FPP, it is widely conjectured that the transversal fluctuations should scale as n<sup>ξ</sup>, where n is the distance and the exponent ξ is equal to 2/3. However, proving this specific scaling rigorously remains a major open problem in the field. The results obtained by Newman and Licea provide rigorous mathematical bounds on these fluctuations, although these bounds are often weaker than the conjectured scaling of n<sup>2/3</sup>. Nevertheless, their work provides valuable information about the shape and behavior of geodesics in FPP, quantifying how much the inherent randomness of the medium forces the optimal paths to deviate from a perfectly straight route 2. This is crucial for developing a comprehensive understanding of the geometry of the random metric space defined by FPP and the nature of the optimal paths within it.7. Infinite Geodesics in First Passage PercolationThe question of whether shortest time paths can extend infinitely in a given direction within the random medium of FPP is a deep and intriguing one. The existence and properties of such infinite geodesics reveal fundamental aspects of the large-scale structure of the model.7.1 Existence and Properties (Hoffman, Damron-Hanson): The existence of infinite geodesics in first passage percolation, meaning shortest paths that originate from a point and extend infinitely in a specific direction, is a subtle and important topic of study. Christopher Hoffman's work has contributed to the understanding of the conditions under which such infinite geodesics can arise. Michael Damron and Jack Hanson have also made significant progress in this area 12. For example, they have shown that in two-dimensional FPP where the edge weights are independent and identically distributed and have a finite first moment (meaning their average value is finite), there exist at least four one-sided infinite geodesics almost surely 16. A one-sided infinite geodesic starts at a particular point and extends infinitely in one direction. The question of the uniqueness of geodesics, both finite between two points and infinite in a given direction, is another related open problem in FPP. It is generally not known whether there is a unique geodesic connecting two given points, especially when the distance between them is large. Interestingly, the question of the non-existence of infinite geodesics in FPP has been found to have connections to the uniqueness of ground states in disordered magnetic systems, such as spin glasses 12. The uniqueness of a ground state in a spin glass system is analogous to the idea that there might be a unique "optimal" way to travel infinitely far in a given direction in FPP. The existence of infinite geodesics reveals fundamental properties of the random metric structure of FPP at very large scales 12. It suggests that there are preferred "directions" of travel within the random medium where one can continue indefinitely along a path that is always locally optimal. The connection between the existence of infinite geodesics in FPP and the uniqueness of ground states in spin glasses highlights a deep analogy between problems in probability theory and statistical physics 12. This suggests that techniques and insights developed in one field might potentially be applicable to the other, fostering a cross-disciplinary approach to understanding complex phenomena in random systems.ConclusionFirst passage percolation, introduced as a model for fluid flow in porous media, has evolved into a central area of study in probability theory. Its simple definition on a lattice with random edge weights gives rise to a rich and complex behavior, leading to numerous deep results and challenging open problems. The model's connections to other areas like directed polymers and last passage percolation, as well as its roots in percolation theory, highlight its fundamental nature. Key properties such as subadditivity lead to the law of large numbers and the existence of a deterministic limit shape. While the Shape Theorem provides a macroscopic understanding of the growth, conjectures about its strict convexity and uniform curvature remain active areas of research. Studies on the concentration and fluctuations of passage times, along with the transversal bounds for geodesics, offer insights into the microscopic behavior and the deviations from the average. Finally, the investigation of infinite geodesics delves into the large-scale structure of the random metric space defined by FPP and reveals connections to problems in statistical physics. The ongoing research in this field continues to uncover new aspects of this fascinating model and its implications across various scientific disciplines.Table 1: Key Historical Milestones in First Passage Percolation
YearEvent/ResultMathematician(s)1965Introduction of the First Passage Percolation modelHammersley and Welsh1969Question posed about the asymptotic behavior of the reachable set B(t)Hammersley and Welsh1981Proof of the Limit Shape TheoremCox and Durrett1987Significant contributions including large deviation estimates and variance boundHarry Kesten2016Existence of a differentiability point on the limit shape (flat edge case)Auffinger and Damron
Table 2: Connections between First Passage Percolation and Other Models
Related Model/PhenomenonNature of ConnectionKey Similarities/DifferencesDirected Polymers in Random MediaZero-temperature limitBoth study paths in random media; FPP focuses on minimum energy, DPRE on statistical mechanics at positive temperature.Last Passage PercolationDual problem with maximization of weightsBoth involve random weights on a graph; different optimization objectives (min vs. max).Eden Growth ModelSimilarities in interface growthFPP focuses on passage times, Eden model on growth rules based on local interactions.Vickrey-Clarke-Groves AuctionsAnalogy in finding minimum cost paths in networksFPP is a probabilistic model, VCG is a mechanism in economics.Disordered Ferromagnets (Spin Glasses)Connection in the existence of infinite geodesics and uniqueness of ground statesAnalogous concepts in random systems with disorder.TASEP (Slow Bond Problem)Related to the columnar defect problem in inhomogeneous FPPConnections between models of random media and interacting particle systems.
