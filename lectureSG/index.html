<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Spin Glass Theory: Lecture Series Notes</title>
    <style>
        body {
            font-family: 'Arial', 'Helvetica Neue', sans-serif;
            max-width: 850px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.7;
            color: #333;
            background-color: #fdfdfd;
        }
        header {
            text-align: center;
            border-bottom: 2px solid #34495e;
            padding-bottom: 20px;
            margin-bottom: 30px;
        }
        h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.2em;
            font-weight: 600;
        }
        .lecture-title {
            color: #7f8c8d;
            font-style: italic;
            margin-bottom: 20px;
            font-size: 1.1em;
        }
        .nav-buttons {
            display: flex;
            flex-wrap: wrap;
            justify-content: center;
            gap: 10px;
            margin: 20px 0;
            padding-bottom: 20px;
            border-bottom: 1px solid #eee;
        }
        .nav-button {
            background-color: #3498db;
            color: white;
            border: none;
            padding: 9px 15px;
            border-radius: 5px;
            cursor: pointer;
            font-size: 14px;
            transition: background-color 0.3s, transform 0.1s;
            font-weight: 500;
        }
        .nav-button:hover {
            background-color: #2980b9;
        }
        .nav-button:active {
            transform: scale(0.98);
        }
        h2 { /* Section Titles */
            color: #2980b9;
            margin-top: 40px;
            border-bottom: 2px solid #ecf0f1;
            padding-bottom: 12px;
            font-size: 1.8em;
            font-weight: 600;
        }
        h3 { /* Lecture Titles */
             color: #2c3e50;
             margin-top: 30px;
             margin-bottom: 15px;
             font-size: 1.5em;
             border-left: 4px solid #3498db;
             padding-left: 10px;
        }
        h4 { /* Sub-sections within Lectures */
            color: #34495e;
            margin-top: 25px;
            margin-bottom: 10px;
            font-size: 1.2em;
            font-weight: 600;
        }
        .section {
            margin-bottom: 30px;
            scroll-margin-top: 20px; /* Adjust scroll position */
            background-color: #fff;
            padding: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
        }
        /* Initially hide all sections */
        .section { display: none; }
        /* Show the first section by default */
        #section1 { display: block; }

        .placeholder {
            background-color: #f9f9f9;
            border-left: 4px solid #bdc3c7;
            padding: 15px;
            color: #7f8c8d;
            font-style: italic;
            margin-top: 20px;
            border-radius: 4px;
        }
        .overview {
            background-color: #eafaf1;
            border-left: 4px solid #2ecc71;
            padding: 10px 15px;
            margin: 15px 0;
            font-style: italic;
            color: #555;
            border-radius: 4px;
        }
        strong, b {
            color: #c0392b; /* Subtle emphasis color */
            font-weight: 600;
        }
        em, i {
            color: #2980b9; /* Subtle emphasis color */
            font-style: italic;
        }
        code, .math.inline { /* Style for inline code/math */
            background-color: #ecf0f1;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.95em;
            color: #555;
        }
        .math.display { /* Style for block math */
            display: block;
            overflow-x: auto; /* Handle long equations */
            margin: 15px 0;
            padding: 10px;
            background-color: #f8f9f9;
            border: 1px solid #eee;
            border-radius: 4px;
            text-align: center; /* Center equations */
        }
        ul, ol {
            margin-left: 20px;
            padding-left: 20px;
        }
        li {
            margin-bottom: 8px;
        }
        hr {
            border: 0;
            height: 1px;
            background-color: #eee;
            margin: 30px 0;
        }
        .reference-list li {
            margin-bottom: 10px;
            font-size: 0.9em;
            color: #555;
        }
        .citation { /* Style for 【1†L128-L135】 */
            font-size: 0.8em;
            color: #95a5a6;
            vertical-align: super;
        }
    </style>
    <!-- MathJax Configuration -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']],
          processEscapes: true,
          processEnvironments: true
        },
        options: {
          skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" id="MathJax-script"></script>
    <script>
        function showSection(sectionId) {
            // Hide all sections
            const sections = document.getElementsByClassName('section');
            for (let i = 0; i < sections.length; i++) {
                sections[i].style.display = 'none';
            }

            // Show the selected section
            const sectionToShow = document.getElementById(sectionId);
            if (sectionToShow) {
                 sectionToShow.style.display = 'block';
                 // Scroll to the section heading smoothly
                 sectionToShow.scrollIntoView({behavior: 'smooth', block: 'start'});
            } else {
                console.error("Section not found:", sectionId);
            }

            // Optional: Highlight active button
            const buttons = document.getElementsByClassName('nav-button');
            for (let i = 0; i < buttons.length; i++) {
                buttons[i].style.backgroundColor = '#3498db'; // Reset color
                if (buttons[i].getAttribute('onclick').includes(`'${sectionId}'`)) {
                    buttons[i].style.backgroundColor = '#2980b9'; // Active color
                }
            }
        }

        // Ensure the first section's button is highlighted on load
        window.onload = function() {
            const firstButton = document.querySelector('.nav-button[onclick*="section1"]');
            if (firstButton) {
                firstButton.style.backgroundColor = '#2980b9';
            }
        }
    </script>
</head>
<body>
    <header>
        <h1>Spin Glass Theory: Lecture Series Notes</h1>
        <p class="lecture-title">Complexity in Disordered Systems</p>

        <div class="nav-buttons">
            <!-- Updated button text to reflect content assignment -->
            <button class="nav-button" onclick="showSection('section1')">Lecture 1: Intro & REM</button>
            <button class="nav-button" onclick="showSection('section2')">Lecture 2: SK Model Intro</button>
            <button class="nav-button" onclick="showSection('section3')">Lectures 3-4: Replica Method & RSB</button>
            <button class="nav-button" onclick="showSection('section4')">Lecture 5: Rigorous Methods</button>
            <button class="nav-button" onclick="showSection('section5')">Lectures 6-7: Cavity & TAP</button>
            <button class="nav-button" onclick="showSection('section6')">Lecture 7 cont: TAP States & Complexity</button>
            <button class="nav-button" onclick="showSection('section7')">Experiments (Placeholder)</button>
            <button class="nav-button" onclick="showSection('section8')">Lecture 8: Applications & AMP</button>
            <button class="nav-button" onclick="showSection('section9')">Research (Placeholder)</button>
            <button class="nav-button" onclick="showSection('section10')">References</button>
        </div>
    </header>

    <div id="section1" class="section">
        <h2>Lecture 1: Background, Motivation, and REM</h2>
        <!-- Content from Markdown Lecture 1 -->
        <h3>Lecture 1: Background and Motivation of Spin Glass Theory</h3>

        <div class="overview">
        <p><strong>Overview:</strong> In this opening lecture, we introduce the concept of spin glasses, discuss why they are important in various fields, and present a fundamental model: the Random Energy Model (REM). This model illustrates key phenomena like <em>quenched vs. annealed disorder</em> and phase transitions in disordered systems, setting the stage for more complex models.</p>
        </div>

        <h4>What is a Spin Glass?</h4>
        <p>A <strong>spin glass</strong> is a disordered magnet where the interactions between spins (magnetic moments) are random in sign and magnitude. Unlike an ordinary ferromagnet (where spins tend to align) or an antiferromagnet (where neighboring spins anti-align), a spin glass has <em>competing interactions</em> (some favor alignment, others favor anti-alignment). This competition leads to <strong>frustration</strong>, meaning not all interaction preferences can be satisfied simultaneously. Physically, spin glasses (such as certain metal alloys) exhibit slow dynamics and many metastable states at low temperatures. Mathematically, they are paradigmatic examples of complex energy landscapes with a vast number of local minima.</p>

        <p><strong>Why study spin glasses?</strong> Beyond condensed matter physics, spin glass models have influenced:</p>
        <ul>
            <li><strong>Probability theory:</strong> providing challenging random structures to develop concentration inequalities and limit theorems.</li>
            <li><strong>Information theory:</strong> inspiring error-correcting codes and inference algorithms by analogy to minimizing a spin glass’s energy (free energy).</li>
            <li><strong>Computer science:</strong> offering insight into the complexity of random optimization problems (e.g. random satisfiability instances) through the lens of energy landscapes and phase transitions.</li>
        </ul>
        <p>Spin glass theory has become an interdisciplinary field, connecting statistical physics with combinatorics, machine learning, and more. In this series, we focus on theoretical developments and rigorous results, while highlighting heuristic methods that have been influential in algorithms.</p>

        <h4>The Random Energy Model (REM)</h4>
        <p>The <strong>Random Energy Model</strong> (REM), introduced by Derrida, is the simplest spin glass model. It assumes an extreme limit of disorder: all possible spin configurations have independent, random energies. Despite its simplicity, REM captures the essence of a spin glass phase transition and provides a testing ground for analytical methods like the second moment method and the replica trick.</p>
        <ul>
            <li><strong>Setup:</strong> Consider $N$ spins with $2^N$ possible configurations (each $\sigma = (\sigma_1,\ldots,\sigma_N)$ with $\sigma_i=\pm1$). Assign each configuration an energy $E(\sigma)$ drawn independently from some distribution. For example, one can take $E(\sigma)$ to be Gaussian with mean $0$ and variance $N$ (so that energies scale extensively with $N$). The system has no structure in its interactions—each state’s energy is random—so all complexity comes from the disorder.</li>
            <li><strong>Partition function:</strong> At inverse temperature $\beta = 1/T$, the partition function is
              <div class="math display">$$Z_N(\beta) \;=\; \sum_{\sigma \in \{\pm1\}^N} e^{-\beta E(\sigma)}.$$</div>
            This random sum encapsulates the competition between entropy (the $2^N$ states) and energy (the Boltzmann weight $e^{-\beta E}$). The <em>quenched free energy</em> (disorder-averaged free energy) per spin is
              <div class="mathdisplay">$$f_N(\beta) \;=\; -\frac{1}{\beta N}\, \mathbb{E}[\ln Z_N(\beta)],$$</div>
            where $\mathbb{E}[\cdot]$ denotes expectation over the random energies (quenched disorder). Under mild conditions, $f_N(\beta)$ converges to a nonrandom limit $f(\beta)$ as $N\to\infty$ (this can be shown via subadditivity arguments, as we discuss later).</li>
        </ul>

        <h5>Second Moment Method in the REM</h5>
        <p>To understand the behavior of $Z_N$, especially the transition from high to low temperature, we apply a <strong>second moment method</strong>. We compare the first and second moments of $Z_N$:</p>
        <ul>
            <li><strong>First moment (annealed approximation):</strong> $\mathbb{E}[Z_N] = \sum_{\sigma} \mathbb{E}[e^{-\beta E(\sigma)}]$. If $E(\sigma)$ has moment-generating function $M_E(t)=\mathbb{E}[e^{tE}]$, then by independence $\mathbb{E}[Z_N] = 2^N M_E(-\beta)$. For example, if $E(\sigma)\sim \mathcal{N}(0,N)$ (normal with mean $0$, variance $N$), then $M_E(-\beta) = \exp(\frac{\beta^2 N}{2})$. In that case,
              <div class="mathdisplay">$$\mathbb{E}[Z_N] = 2^N \exp\!\Big(\frac{\beta^2 N}{2}\Big) = \exp\!\Big(N [\ln 2 + \frac{\beta^2}{2}]\Big).$$</div>
            The <em>annealed free energy</em> (treating the disorder as if it were averaged out) per spin would be
              <div class="mathdisplay">$$f_{\text{ann}}(\beta) = -\frac{1}{\beta N} \ln \mathbb{E}[Z_N] = -\frac{1}{\beta}(\ln 2 + \frac{\beta^2}{2}).$$</div>
            More generally, for small $\beta$ the dominant contribution to $\mathbb{E}[Z_N]$ is $2^N$ (all configurations contribute), reflecting high entropy.</li>
            <li><strong>Second moment:</strong> $\mathbb{E}[Z_N^2] = \mathbb{E}\big[(\sum_{\sigma} e^{-\beta E(\sigma)})^2\big] = \sum_{\sigma,\tau} \mathbb{E}[e^{-\beta E(\sigma)} e^{-\beta E(\tau)}]$. By independence, $\mathbb{E}[e^{-\beta E(\sigma)} e^{-\beta E(\tau)}] = \mathbb{E}[e^{-2\beta E}]$ if $\sigma=\tau$, and $[\mathbb{E}(e^{-\beta E})]^2$ if $\sigma\neq\tau$. Thus,
              <div class="mathdisplay">$$\mathbb{E}[Z_N^2] = 2^N \mathbb{E}[e^{-2\beta E}] + 2^N(2^N-1)\,[\mathbb{E}(e^{-\beta E})]^2.$$</div>
            For the Gaussian example: $\mathbb{E}[e^{-2\beta E}] = \exp(2\beta^2 N/2) = \exp(\beta^2 N)$ and $\mathbb{E}[e^{-\beta E}] = \exp(\beta^2 N/2)$. Then
              <div class="mathdisplay">$$\mathbb{E}[Z_N^2] \approx 2^N e^{\beta^2 N} + 2^{2N} e^{\beta^2 N} = 2^{2N} e^{\beta^2 N} (2^{-N} + 1).$$</div>
            Meanwhile $(\mathbb{E}[Z_N])^2 = 2^{2N} e^{\beta^2 N}$. So
              <div class="mathdisplay">$$\frac{\mathbb{E}[Z_N^2]}{(\mathbb{E}[Z_N])^2} \approx 1 + 2^{-N}.$$</div>
            In this case the ratio tends to 1 for large $N$, indicating that $Z_N$ is sharply concentrated around its mean. In contrast, if for some $\beta$ the second moment grows much larger than the square of the first, it signals that $Z_N$ is dominated by rare low-energy configurations and has large fluctuations.</li>
            <li><strong>Implications:</strong> When $\mathbb{E}[Z_N^2] / (\mathbb{E}[Z_N])^2 \to 1$ as $N\to\infty$, the <em>annealed approximation</em> is essentially accurate (self-averaging regime). This tends to be true at sufficiently high temperatures. On the other hand, if the ratio grows exponentially with $N$, the partition function is dominated by a few lowest-energy terms and annealing fails. In that case, the quenched free energy differs significantly from the annealed free energy, indicating a <em>glassy phase</em>.</li>
        </ul>

        <h5>Phase Transition in the REM</h5>
        <p>In the REM, as $\beta$ increases (temperature decreases), there is a critical $\beta_c$ at which the behavior changes from annealed-like to glassy. Qualitatively:</p>
        <ul>
            <li>For $\beta < \beta_c$ (high $T$): Many configurations contribute comparably to $Z_N$. The free energy is dominated by entropy: $f(\beta) \approx -\frac{1}{\beta}\ln 2$ (plus small energy corrections). Disorder is weak in effect, and $\mathbb{E}[\ln Z_N] \approx \ln \mathbb{E}[Z_N]$ for large $N$.</li>
            <li>For $\beta > \beta_c$ (low $T$): A tiny fraction of the lowest-energy states dominate $Z_N$. The free energy approaches the ground state energy per spin (up to thermal fluctuations). The system is in a spin glass phase: thermal weight concentrates on rare states, and sample-to-sample fluctuations of $\ln Z_N$ are large.</li>
        </ul>
        <p><strong>Critical point:</strong> $\beta_c$ is found by equating the entropy lost by focusing on rare states with the energy gained. In our Gaussian example, one finds $\beta_c = \sqrt{2 \ln 2}$ (if variance is $N$, this corresponds to $\beta_c= \sqrt{2 \ln 2 / \sigma^2}$ if $\sigma^2=1$). In Derrida’s original REM (with a specific exponential-tail energy distribution), $\beta_c = \sqrt{2\ln 2}$. For $\beta > \beta_c$, the annealed calculation $f_{\text{ann}}(\beta)$ overestimates the true free energy; the correct $f(\beta)$ is lower (more negative free energy, meaning a lower energy state is realized).</p>
        <p>The REM thus has a <strong>one-step replica symmetry breaking</strong> (1RSB) character: above $\beta_c$, the system is in one “ergodic” state (disordered), while below $\beta_c$ it freezes into one of an exponential number of possible ground states. However, since these states are completely uncorrelated in REM, the model is in a sense <em>trivial</em> beyond the transition (there is no hierarchy or overlap structure between states, just one picked at random to dominate).</p>

        <p><strong>Key takeaways:</strong> The REM illustrates how quenched disorder can lead to a fundamental difference between averaging inside the log (quenched) and outside (annealed). At high $T$, disorder averages out (annealed ≈ quenched), whereas at low $T$ the system exhibits a glassy phase where rare events dominate. In most non-trivial spin glasses, indeed <em>the annealed free energy does not equal the quenched free energy except in a trivial high-temperature regime</em><span class="citation">【1†L128-L135】</span>. The REM provides a baseline for understanding more complex spin glasses like the one we discuss next.</p>

        <p><strong>Summary (Lecture 1):</strong> We introduced spin glasses as systems with competing random interactions, motivating their study across disciplines. We examined the Random Energy Model, which showed a sharp transition from a paramagnetic (entropy-dominated) phase to a frozen (energy-dominated) phase. This simple model highlights the necessity of quenched analysis (as opposed to annealed) in disordered systems. In the next lecture, we turn to the more structured Sherrington–Kirkpatrick model, which requires deeper ideas to solve.</p>
        <!-- End of Lecture 1 Content -->
    </div>

    <div id="section2" class="section">
        <h2>Lecture 2: The Sherrington-Kirkpatrick (SK) Model Introduction</h2>
        <!-- Content from Markdown Lecture 2 -->
        <h3>Lecture 2: Background and Motivation of the SK Model</h3>

        <div class="overview">
        <p><strong>Overview:</strong> In this lecture, we introduce the Sherrington–Kirkpatrick (SK) model, the paradigm of a mean-field spin glass with pairwise interactions. We discuss its thermodynamics, the difficulties that arise at low temperature, and set the stage for solution methods (which will follow in subsequent lectures). The audience is assumed to have a basic background in probability theory, Gaussian processes, entropy, and measure concentration.</p>
        </div>

        <h4>Definition and Mean-Field Nature of the SK Model</h4>
        <p>The SK model consists of $N$ Ising spins with all-to-all random couplings:</p>
        <ul>
            <li><strong>Hamiltonian:</strong>
              <div class="mathdisplay">$$H_N(\sigma) \;=\; -\frac{1}{\sqrt{N}}\sum_{1\le i<j\le N} J_{ij}\,\sigma_i \sigma_j,$$</div>
            where $\sigma_i = \pm 1$ and $J_{ij}$ are i.i.d. random couplings (usually $J_{ij}\sim \mathcal{N}(0,1)$). The $1/\sqrt{N}$ scaling is chosen so that the extensive free energy is $O(N)$ as $N\to\infty$. Notice every spin interacts with every other; this infinite-range model is a <strong>mean-field</strong> spin glass.</li>
            <li><strong>Disorder:</strong> $\{J_{ij}\}$ is quenched disorder. On average $J_{ij}=0$, so there is no bias toward any particular spin orientation pattern. The randomness is symmetric under $\sigma \to -\sigma$ (global spin flip), since the energy depends on products $\sigma_i\sigma_j$. We consider the case of zero external magnetic field.</li>
            <li><strong>Overlap order parameter:</strong> As in any spin glass, the appropriate order parameter is not the magnetization (which is zero on average here), but the distribution of overlaps between two equilibrium configurations. For configurations $\sigma$ and $\tau$, the overlap is
              <div class="mathdisplay">$$q(\sigma,\tau) \;=\; \frac{1}{N}\sum_{i=1}^N \sigma_i \tau_i.$$</div>
            For two independent random configurations (each drawn uniformly), $q\approx 0$. If $\tau=\sigma$, $q=1$. Overlap measures similarity between states and plays the role of an order parameter in spin glasses (replacing magnetization used in ferromagnets).</li>
            <li><strong>Thermodynamics:</strong> The partition function is $Z_N = \sum_{\sigma} e^{-\beta H_N(\sigma)}$. We are interested in the quenched free energy $f_N(\beta) = -\frac{1}{\beta N}\mathbb{E}[\ln Z_N]$ and its $N\to\infty$ limit $f(\beta)$. Because of the mean-field nature, each spin feels a cumulative random field from all other spins. At high temperatures, one expects spins to essentially randomize (paramagnetic behavior, with zero average magnetization and low overlap between independent samples). At low temperatures, the system may enter a spin glass phase with many possible frozen patterns of spins (metastable states) and nonzero typical overlap between identical copies of the system (replicas).</li>
        </ul>

        <h4>High-Temperature Phase (Replica Symmetric Phase)</h4>
        <p>At very high temperatures (small $\beta$), the SK model behaves like a paramagnet with a slight perturbation from random bonds. All spins are essentially disorderly, and the effect of the random $J_{ij}$ is averaged out by thermal fluctuations. In this regime:</p>
        <ul>
            <li>The <em>annealed</em> and <em>quenched</em> free energies coincide to leading order. For example, expanding $\ln Z_N$ in powers of $\beta$, one finds $\mathbb{E}[\ln Z_N] = N \ln 2 + \frac{\beta^2 N}{4} + O(\beta^4)$, which matches $\ln \mathbb{E}[Z_N]$ at small $\beta$. In general, for $\beta$ below a certain threshold, fluctuations of $\ln Z_N$ around its mean are negligible (concentration of measure holds).</li>
            <li>The only solution to the mean-field equations is the trivial one: all spin magnetizations $\langle \sigma_i\rangle = 0$. In other words, the <strong>replica symmetric (RS) solution</strong> (assuming the system is in essentially one state with symmetry intact) is valid. There is no spontaneous magnetization or symmetry breaking. Correlations between two independent samples (replicas) are small: $\mathbb{E}[q(\sigma,\tau)] \approx 0$ for $\sigma,\tau$ from independent thermal runs.</li>
        </ul>
        <p>More formally, one can show that for high $T$ the Gibbs measure is <em>unique</em> and resembles a product measure with small correlations. The spin glass susceptibility (which measures response to a symmetry-breaking field) is finite.</p>

        <p><strong>The critical temperature:</strong> As $\beta$ increases, this paramagnetic solution eventually becomes unstable. The point of instability can be identified by analyzing when a small perturbation (like a small field or a small assumed overlap) can sustain itself. For the SK model, this happens at $\beta_c = 1$ (in suitable units for the Gaussian SK model). At $\beta_c$, the spin glass susceptibility diverges, indicating the onset of a collective freezing of spins into random aligned patterns.</p>
        <p>Above $T_c = 1/\beta_c$, the system is in the <strong>replica symmetric</strong> regime (no symmetry breaking). Below $T_c$, the system enters a spin glass phase where replica symmetry is broken (as we will see, one must go beyond the RS ansatz to describe the state).</p>

        <h4>The Dilemma of the Low-Temperature Phase</h4>
        <p>Sherrington and Kirkpatrick attempted to solve the model by assuming a single “effective state” even at low $T$ (the <strong>replica symmetric</strong> ansatz, which we will discuss next). They found an analytic expression for the free energy in that assumption. Strikingly, their solution showed unphysical behavior at very low temperatures (negative entropy, which is impossible). This indicated that the replica symmetric assumption was incomplete — the symmetry between different replicas (independent samples of the system) must be broken in the true solution.</p>
        <p>It was Giorgio Parisi in 1979 who proposed a groundbreaking <em>replica symmetry breaking (RSB)</em> scheme to fix this. Parisi's ansatz introduced an infinite hierarchy of states and order parameters, ultimately yielding a consistent solution with positive entropy and novel properties (like a continuous distribution of overlaps between pure states). Parisi’s solution was later shown to be correct, but its derivation was non-rigorous at the time.</p>
        <p>For now, the SK model stands as a central object in spin glass theory: it exhibits a complex low-temperature phase that cannot be described by any single order parameter like magnetization. Instead, an <strong>order parameter distribution</strong> (the overlap distribution $P(q)$) is needed. This model has driven the development of sophisticated analytical methods (replicas, cavity, TAP equations) and inspired rigorous mathematical breakthroughs to understand its free energy landscape.</p>

        <p><strong>Summary (Lecture 2):</strong> We introduced the SK model, a mean-field spin glass with infinitely many random couplings. At high temperature, the model is paramagnetic and replica symmetric (no complicated state structure). At low temperature, the naive approach (replica symmetric solution) leads to paradoxes (like negative entropy), hinting at a much richer structure. This sets the stage for the next lectures: we will explore analytical methods to solve the SK model, including the replica method with Parisi’s RSB ansatz (Lectures 3–4), rigorous comparison and interpolation techniques (Lecture 5), and the cavity/TAP approaches (Lectures 6–7).</p>
        <!-- End of Lecture 2 Content -->
    </div>

    <div id="section3" class="section">
        <h2>Lectures 3 & 4: The Replica Method and Replica Symmetry Breaking (RSB)</h2>
        <!-- Content from Markdown Lectures 3 & 4 -->
        <h3>Lecture 3: The Replica Method – Basics and REM</h3>

        <div class="overview">
        <p><strong>Overview:</strong> The replica method is a formal technique to handle disorder averages of logarithms, widely used in spin glass theory. In this lecture, we introduce the replica trick and demonstrate it on a simple model (the Random Energy Model). We derive how the replica method reproduces the REM results (including the phase transition) from Lecture 1. This will set the stage to tackle the more complex Sherrington–Kirkpatrick (SK) model in the next lecture.</p>
        </div>

        <h4>The Replica Trick</h4>
        <p>In disordered systems, one often needs to calculate an average of a logarithm (e.g. $\mathbb{E}[\ln Z]$ for the free energy). The replica trick introduces the relation:</p>
        <div class="mathdisplay">$$ \mathbb{E}[\ln Z] = \lim_{n\to 0} \frac{\mathbb{E}[Z^n] - 1}{n}\,. $$</div>
        <p>Here $n$ is an auxiliary variable, initially an integer so that $\mathbb{E}[Z^n]$ is a usual moment. After deriving a formula for $\mathbb{E}[Z^n]$ for $n \in \mathbb{N}$, one analytically continues to real $n$ and takes $n\to 0$. While treating $n$ as a real variable is not rigorously justified, this technique has led to correct predictions in many problems.</p>
        <p>To use the replica method:</p>
        <ol>
            <li><strong>Compute $\mathbb{E}[Z^n]$ for integer $n$:</strong> This typically involves writing
              <div class="mathdisplay">$$Z^n = Z(\beta; J_1) Z(\beta; J_2)\cdots Z(\beta; J_n),$$</div>
            i.e. $n$ copies (replicas) of the partition function with the <em>same</em> disorder realization $J$. Then $\mathbb{E}[Z^n] = \mathbb{E}_{J}[Z(J)^n]$. Expanding $Z^n$ as a sum over configurations of each replica leads to expressions involving products of disorder terms, which can be averaged using independence or known cumulants.</li>
            <li><strong>Assume symmetry (or symmetry breaking) among replicas:</strong> All replicas are statistically identical, so one often assumes they behave similarly. For example, one might assume all pairwise overlaps between replicas are equal (replica symmetry) unless there is reason to break that assumption.</li>
            <li><strong>Evaluate or extremize the resulting expression:</strong> After averaging over disorder, $\mathbb{E}[Z^n]$ may become an integral (or sum) that can be tackled by saddle-point (steepest descent) methods for large $N$. One identifies the dominant contribution (extremum of an exponent).</li>
            <li><strong>Take $n\to 0$:</strong> Expand the result for small $n$ and extract the coefficient of $n$ (since $\frac{1}{n}(\mathbb{E}[Z^n]-1)$ will yield that coefficient as $n\to 0$). This gives $\mathbb{E}[\ln Z]$.</li>
        </ol>
        <p>The tricky step is often step 2: choosing the correct ansatz for symmetry (or symmetry breaking) among replicas. If the wrong ansatz is chosen, one might get an unphysical result (like negative entropy), signaling the need for a more complex symmetry-breaking ansatz.</p>

        <h4>Replica Analysis of the REM</h4>
        <p>We illustrate the replica method on the <strong>Random Energy Model</strong> (REM). Recall: $Z_N(\beta) = \sum_{i=1}^{2^N} e^{-\beta E_i}$ with i.i.d. energies $E_i$. We want $\mathbb{E}[\ln Z_N]$.</p>
        <p>Using the replica trick:</p>
        <div class="mathdisplay">$$ \mathbb{E}[Z_N^n] = \mathbb{E}\Big[\Big(\sum_{i=1}^{2^N} e^{-\beta E_i}\Big)^n\Big] = \sum_{i_1,\dots,i_n} \mathbb{E}[e^{-\beta (E_{i_1}+\cdots+E_{i_n})}]\,. $$</div>
        <p>The average factorizes: if all $i_a$ are distinct, $\mathbb{E}[e^{-\beta\sum_a E_{i_a}}] = [\mathbb{E}(e^{-\beta E})]^n$; if some indices coincide, the expectation involves higher moments.</p>
        <p>We can categorize terms by how many replicas share the same configuration:</p>
        <ul>
            <li><strong>All replicas in distinct states:</strong> There are $2^N (2^N-1)\cdots(2^N-n+1) \approx (2^N)^n$ terms. Each contributes $[\mathbb{E}(e^{-\beta E})]^n$. So $\ln \mathbb{E}[Z^n] \approx n\ln(2^N \mathbb{E}(e^{-\beta E})) = n (N\ln 2 + \ln M_E(-\beta))$. For the Gaussian example (variance $N$, $\sigma^2=1$), this gives $\approx nN(\ln 2 + \beta^2/2)$.</li>
            <li><strong>All replicas in the same state:</strong> There are $2^N$ choices for the state. The contribution is $\mathbb{E}[e^{-n\beta E}] = M_E(-n\beta)$. Taking $\ln$ gives $\ln(2^N) + \ln M_E(-n\beta) = N \ln 2 + \ln M_E(-n\beta)$. For small $n$, expand $\ln M_E(-n\beta) \approx \frac{(n\beta)^2 N}{2}$ (for mean 0, variance $N$ energies).</li>
        </ul>
        <p>For large $N$, the dominant contribution to $\mathbb{E}[Z^n]$ comes from the term with the largest exponent. Compare exponents per spin $\frac{1}{N}\ln(\cdot)$:</p>
        <ul>
            <li>Distinct states: exponent $\approx n [\ln 2 + \frac{\beta^2}{2}]$.</li>
            <li>Same state: exponent $\approx \ln 2 + \frac{(n\beta)^2}{2}$.</li>
        </ul>
        <p>For small $n \to 0$, we need $\lim_{n\to0} \frac{1}{nN}\ln \mathbb{E}[Z_N^n]$. The term linear in $n$ dominates.</p>
        <ul>
             <li>If $\beta < \beta_c = \sqrt{2\ln 2}$ (using $\sigma^2=1$), the distinct states term gives the dominant linear coefficient $\ln 2 + \beta^2/2$. So, the quenched free energy per spin is $f(\beta) = -\frac{1}{\beta} \lim_{n\to 0} \frac{\ln \mathbb{E}[Z_N^n]}{nN} = -\frac{1}{\beta}(\ln 2 + \beta^2/2)$, which matches the annealed result.</li>
             <li>If $\beta > \beta_c$, the annealed calculation leads to issues (like positive free energy if naively continued). The correct physics is that the lowest energy states dominate. In the replica calculation, this corresponds to the "same state" contribution becoming relevant, or more accurately, the replica symmetry breaking. A careful analysis (often involving finding the point where the annealed entropy vanishes) shows that for $\beta > \beta_c$, the free energy freezes at its value at $\beta_c$, related to the minimum energy density. $f(\beta) = -\frac{1}{\beta_c}(\ln 2 + \beta_c^2/2) = - \sqrt{2 \ln 2}$ (for $\sigma^2=1$).</li>
        </ul>

        <p>This matches the known REM solution: high-temperature (replica symmetric) free energy and a frozen low-temperature free energy. Notably, our replica calculation implicitly involved replica symmetry breaking for $\beta > \beta_c$ when we recognized the failure of the symmetric (annealed-like) solution.</p>

        <p><strong>Summary (Lecture 3):</strong> We introduced the replica trick and applied it to the REM, finding results consistent with earlier methods. We saw how the method requires breaking symmetry between replicas when a phase transition occurs (distinct vs. collapsed replica contributions). In the next lecture, we will apply these ideas to the SK model. We will see the replica symmetric solution for SK and why it fails at low temperatures, leading to Parisi’s replica symmetry breaking scheme. We’ll also consider an application of the replica method to a neural network model (the perceptron) to show its broad reach.</p>

        <hr>

        <h3>Lecture 4: Replica Method for the SK Model and Beyond</h3>

        <div class="overview">
        <p><strong>Overview:</strong> Building on the previous lecture, we now apply the replica method to the SK model. We derive the replica symmetric (RS) solution and discuss its instability. We then outline Parisi’s replica symmetry breaking (RSB) ansatz that resolved the problems of the RS solution. Finally, we highlight an application of replicas to the perceptron model (a simple neural network), demonstrating the method’s use outside physics.</p>
        </div>

        <h4>Replica Calculation for the SK Model (RS Ansatz)</h4>
        <p>For the SK model, the partition function is</p>
        <div class="mathdisplay">$$Z_N(\beta) = \sum_{\sigma} e^{-\beta H_N(\sigma)} = \sum_{\sigma}\exp\Big(\beta \frac{1}{\sqrt{N}}\sum_{i<j} J_{ij}\sigma_i\sigma_j\Big).$$</div>
        <p>To compute $\mathbb{E}[\ln Z_N]$, we write $\mathbb{E}[Z_N^n]$ for integer $n$:</p>
        <div class="mathdisplay">$$ \mathbb{E}[Z^n] = \mathbb{E}_{J}\Big[\sum_{\{\sigma^a\}_{a=1}^n} \exp\Big(\beta \frac{1}{\sqrt{N}}\sum_{i<j}\sum_{a=1}^n J_{ij}\,\sigma_i^a \sigma_j^a\Big)\Big]. $$</div>
        <p>Swap the sum and expectation. The disorder average factorizes using $\mathbb{E}[e^{x J_{ij}}] = \exp(\frac{1}{2} x^2 \mathrm{Var}(J_{ij}))$. Since $J_{ij} \sim \mathcal{N}(0,1)$, the term in the exponent is $\frac{\beta}{\sqrt{N}} \sum_{a} \sigma_i^a \sigma_j^a$. The variance is $1$. So averaging over $J_{ij}$ gives:</p>
        <div class="mathdisplay">$$ \mathbb{E}[Z^n] = \sum_{\{\sigma^a\}} \exp\Big(\frac{\beta^2}{2N}\sum_{i<j}\Big(\sum_{a=1}^n \sigma_i^a \sigma_j^a\Big)^2\Big). $$</div>
        <p>We introduce the overlaps</p>
        <div class="mathdisplay">$$q_{ab} = \frac{1}{N}\sum_{i=1}^N \sigma_i^a \sigma_i^b$$</div>
        <p>for each replica pair $(a,b)$. Note $q_{aa}=1$. Then one can show (after some algebra):</p>
        <div class="mathdisplay">$$ \sum_{i<j}\Big(\sum_{a=1}^n \sigma_i^a \sigma_j^a\Big)^2 \approx \frac{N^2}{2}\sum_{a\neq b} q_{ab}^2 + \text{constant terms}. $$</div>
        <p>So:</p>
        <div class="mathdisplay">$$ \mathbb{E}[Z^n] \approx \sum_{\{\sigma^a\}} \exp\Big(\frac{\beta^2 N}{4}\sum_{a\neq b} q_{ab}^2 \Big). $$</div>
        <p>The next step involves a saddle-point method over the configurations, often transformed into an integral over the overlap parameters $q_{ab}$ using delta functions or Hubbard-Stratonovich transformations. This leads to an extremization problem for an effective free energy functional $\Phi(\{q_{ab}\})$:</p>
        <div class="mathdisplay">$$ \mathbb{E}[Z^n] \approx \int \prod_{a<b} dq_{ab} \,\exp\{-N \Psi(\{q_{ab}\})\} $$</div>
        <p>where $\Psi$ contains terms involving $\frac{\beta^2}{4} \sum q_{ab}^2$ and entropic contributions derived from counting configurations with given overlaps.</p>

        <p>Under the <strong>replica symmetric (RS) ansatz</strong>, we assume all off-diagonal overlaps are equal: $q_{ab} = q$ for all $a\neq b$. This simplifies the problem dramatically. Carrying out the calculation and taking the $n\to 0$ limit yields the RS free energy per spin $f_{RS}(q)$. Extremizing this with respect to $q$ gives the famous self-consistency equation:</p>
        <div class="mathdisplay">$$ q = \int_{-\infty}^{\infty} \frac{dz}{\sqrt{2\pi}} e^{-z^2/2} \tanh^2(\beta \sqrt{q}\, z)\,. $$</div>
        <p>(Note: The exact derivation involves integrating over auxiliary fields and is more complex than sketched here). Solving this equation:</p>
        <ul>
            <li>For $\beta < 1$: $q=0$ is the only stable solution (paramagnetic phase).</li>
            <li>For $\beta > 1$: A non-zero solution $q > 0$ appears, suggesting an ordered (spin glass) phase. However, this RS solution leads to a negative entropy $S = -\partial f_{RS}/\partial T$ as $T\to 0$, which is unphysical.</li>
        </ul>
        <p>Thus, the RS ansatz works for $\beta < 1$, but fails for $\beta > 1$. The failure indicates the need for Replica Symmetry Breaking (RSB).</p>

        <h4>Parisi’s Replica Symmetry Breaking (RSB) Ansatz</h4>
        <p>Giorgio Parisi (1979) proposed that below $T_c$, the symmetry between replicas must be broken hierarchically:</p>
        <ul>
            <li>Instead of a single $q$, allow a structured matrix $q_{ab}$. The simplest is <strong>one-step RSB (1RSB)</strong>: divide $n$ replicas into $m$ groups of size $n/m$. Set $q_{ab} = q_1$ if $a,b$ are in the same group ($a\neq b$), and $q_{ab} = q_0$ if in different groups. Take $n\to 0$ (making $m$ a parameter in $[0,1]$) and extremize over $q_0, q_1, m$.</li>
            <li>If 1RSB is unstable, break groups further (2RSB), and so on. For the SK model, an infinite sequence of breakings (<strong>full RSB</strong>) is needed.</li>
        </ul>
        <p>Parisi’s full RSB ansatz is described by a continuous function $q(x)$ for $x\in[0,1]$, representing overlap distribution. The theory predicts:</p>
        <ul>
            <li>A non-trivial overlap distribution $P(q)$ with support on an interval $[q_{\min}, q_{\max}]$.</li>
            <li>Physically correct results (e.g., positive entropy).</li>
            <li><strong>Ultrametricity</strong>: The space of pure states has a tree-like structure.</li>
        </ul>
        <p>Parisi’s scheme yielded a free energy value later proven to be correct for the SK model.</p>

        <h4>Example Application: Replica Method in the Perceptron Model</h4>
        <p>Spin glass techniques apply to machine learning. Example: the storage capacity of a perceptron. Given $P$ random patterns $\{\xi^\mu\}_{\mu=1}^P \in \{\pm1\}^N$ and labels $y^\mu=\pm1$, find the maximum ratio $\alpha = P/N$ such that a weight vector $w$ exists with $\operatorname{sign}(w\cdot \xi^\mu) = y^\mu$ for all $\mu$.</p>
        <p>Gardner (1988) used the replica method (often with RS or 1RSB) to calculate this capacity $\alpha_c$. For instance, for spherical weights $\|w\|^2=N$, $\alpha_c = 2$. This demonstrates the broad applicability of replica methods beyond physics.</p>

        <p><strong>Summary (Lecture 4):</strong> We applied the replica method to SK, derived the RS solution $q = \int dz \, \mathcal{N}(z;0,1) \tanh^2(\beta\sqrt{q}z)$, and noted its failure at low $T$. Parisi's RSB provides the correct description via a hierarchical breaking of symmetry, described by $q(x)$ or $P(q)$. The method's power extends to problems like the perceptron capacity.</p>
        <!-- End of Lectures 3 & 4 Content -->
    </div>

    <div id="section4" class="section">
        <h2>Lecture 5: Rigorous Methods - Gaussian Comparison & Guerra's Interpolation</h2>
        <!-- Content from Markdown Lecture 5 -->
        <h3>Lecture 5: Gaussian Comparison and Guerra’s Interpolation</h3>

        <div class="overview">
        <p><strong>Overview:</strong> In this lecture, we shift focus to rigorous tools developed for spin glasses. We discuss <strong>Gaussian comparison inequalities</strong> (like Slepian’s and Gordon’s lemmas) which compare maxima/free energies of Gaussian systems. We then introduce <strong>Guerra’s interpolation method</strong>, a technique to derive bounds on the free energy by interpolating between simple and complex models, crucial for proving the validity of the Parisi solution.</p>
        </div>

        <h4>Existence of the Thermodynamic Limit (Subadditivity)</h4>
        <p>A fundamental question is whether the free energy per spin $f_N(\beta) = -\frac{1}{\beta N}\mathbb{E}[\ln Z_N]$ converges as $N\to\infty$. This was proven using subadditivity arguments.</p>
        <ul>
            <li><strong>Heuristic:</strong> Combining two independent systems of size $N$ and $M$ non-interactively gives $\mathbb{E}[\ln Z_{N+M}] = \mathbb{E}[\ln Z_N] + \mathbb{E}[\ln Z_M]$. Adding interactions between them typically constrains the system, potentially lowering $Z$. This suggests superadditivity for $\mathbb{E}[\ln Z_N]$ (or subadditivity for $-\mathbb{E}[\ln Z_N]$). Fekete’s Lemma then implies convergence of the quantity per spin.</li>
            <li><strong>Guerra–Toninelli (2002):</strong> Provided a rigorous proof for mean-field models like SK, establishing the existence of the limit $f(\beta) = \lim_{N\to\infty} f_N(\beta)$<span class="citation">【20†L51-L59】</span>. This was a key step towards rigorously validating the Parisi solution.</li>
        </ul>

        <h4>Gaussian Comparison Principles</h4>
        <p>These principles relate the extrema of different Gaussian processes based on their covariance structure.</p>
        <ul>
            <li><strong>Slepian’s Lemma:</strong> If $(X_i)$ and $(Y_i)$ are centered Gaussian processes with $\mathbb{E}[X_i^2] = \mathbb{E}[Y_i^2]$ and $\mathbb{E}[X_i X_j] \le \mathbb{E}[Y_i Y_j]$ for $i\neq j$, then $\mathbb{E}[\max_i X_i] \le \mathbb{E}[\max_i Y_i]$. More positive correlation tends to increase the maximum.</li>
            <li><strong>Gordon’s Inequality (Gaussian Min-Max Theorem):</strong> Relates the extremum of a Gaussian process to that of a simpler, related process. Useful for comparing complex models to simpler ones.</li>
        </ul>
        <p><strong>Application to spin glasses:</strong> The Hamiltonian $H_N(\sigma)$ can be viewed as a Gaussian process indexed by $\sigma$.</p>
        <ul>
            <li>SK Model: $\mathbb{E}[H_N(\sigma)H_N(\tau)] \propto (q_{\sigma\tau})^2$.</li>
            <li>REM: $\mathbb{E}[H_N(\sigma)H_N(\tau)] = 0$ for $\sigma \neq \tau$ (less correlated).</li>
            <li>Ferromagnet (CW): $\mathbb{E}[H_N(\sigma)H_N(\tau)] \propto q_{\sigma\tau}$ (more correlated).</li>
        </ul>
        <p>Slepian's lemma implies relationships between ground state energies (maxima of $-H$) of these models. While direct application gives crude bounds on free energy, refined uses (e.g., comparing SK to spherical models via Gordon's inequality) yield useful rigorous results without assuming replica symmetry.</p>

        <h4>Guerra’s Interpolation Method</h4>
        <p>Introduced by Francesco Guerra (2003), this powerful technique rigorously proves that the Parisi formula gives an upper bound on the true SK free energy.</p>
        <p><strong>Idea:</strong> Define an interpolating Hamiltonian $H(t)$ between a simple reference model $H^{(0)}$ (e.g., non-interacting spins in random fields) and the SK model $H^{SK}$:</p>
        <div class="mathdisplay">$$ H(t) = \sqrt{t}\,H^{SK}(\sigma) + \sqrt{1-t}\,H^{(0)}(\sigma)\,. $$</div>
        <p>Let $F_N(t) = \frac{1}{N}\mathbb{E}[\ln Z(t)]$ be the free energy density at interpolation parameter $t$. We want $F_N(1)$. Using the fundamental theorem of calculus:</p>
        <div class="mathdisplay">$$ F_N(1) - F_N(0) = \int_0^1 \frac{d}{dt} F_N(t)\, dt\,. $$</div>
        <p>Guerra calculated the derivative $\frac{d}{dt}F_N(t)$ using Gaussian integration by parts:</p>
        <div class="mathdisplay">$$ \frac{d}{dt}F_N(t) = \text{Expression involving overlaps } \langle q_{ab}^k \rangle_t \text{ under } H(t). $$</div>
        <p>For the SK model (interpolating from a simple paramagnet), the derivative takes the form:</p>
        <div class="mathdisplay">$$ \frac{d}{dt}F_N(t) = -\frac{\beta^2}{4}\Big(1 - \mathbb{E}\langle q_{12}^2 \rangle_t\Big)\,, $$</div>
        <p>where $\langle q_{12}^2 \rangle_t$ is the expected squared overlap between two replicas drawn from the Gibbs measure defined by $H(t)$.</p>
        <p><strong>Key Insight:</strong> Guerra showed that this derivative is related to the Parisi functional. By choosing the reference $H^{(0)}$ carefully to match a specific RSB structure (defined by a trial function $q(x)$ or a discrete RSB scheme), $F_N(0)$ becomes exactly the Parisi free energy functional evaluated at that trial structure.</p>
        <p>Furthermore, under suitable conditions, one can show that $\frac{d}{dt}F_N(t)$ matches the derivative term in the Parisi PDE, or prove bounds on it. A crucial result is that if one chooses $H^{(0)}$ corresponding to the *optimizing* Parisi solution, the interpolation argument leads to:</p>
        <div class="mathdisplay">$$ F_{SK} \le F_{\text{Parisi}} $$</div>
        <p>This is **Guerra’s bound**<span class="citation">【20†L55-L63】</span>: the true SK free energy is bounded above by the value obtained from Parisi's replica calculations. This was a major step in proving the correctness of the Parisi solution.</p>
        <ul>
             <li>Later work by Talagrand provided the matching lower bound $F_{SK} \ge F_{\text{Parisi}}$, confirming $F_{SK} = F_{\text{Parisi}}$.</li>
        </ul>

        <p><strong>Summary (Lecture 5):</strong> Rigorous tools like Gaussian comparison inequalities and Guerra's interpolation method provide ways to analyze spin glasses without relying on the replica heuristic. Subadditivity proves the existence of the thermodynamic limit. Guerra's interpolation established that the Parisi formula gives an upper bound on the true free energy, paving the way for a full proof of the Parisi solution for the SK model.</p>
        <!-- End of Lecture 5 Content -->
    </div>

    <div id="section5" class="section">
        <h2>Lectures 6 & 7: The Cavity Method and TAP Equations</h2>
        <!-- Content from Markdown Lectures 6 & 7 -->
        <h3>Lecture 6: The Cavity Method – Intuition and High-Temperature Analysis</h3>

        <div class="overview">
        <p><strong>Overview:</strong> The cavity method, developed by Mézard, Parisi, and others, offers a more physical approach to spin glasses. It analyzes a system by considering the effect of removing or adding a single spin (the "cavity"). This lecture introduces the cavity intuition and applies it to the SK model in the high-temperature (replica-symmetric) phase, deriving self-consistent equations.</p>
        </div>

        <h4>Basic Idea of the Cavity Method</h4>
        <p>Consider an $N$-spin system. Remove spin $N$ to create a "cavity" system of $N-1$ spins. The removed spin influenced others via $J_{i,N}$. Conversely, adding spin $N$ back to the $(N-1)$-spin system means it experiences an effective field from the cavity spins.</p>
        <p>For the SK model:</p>
        <ul>
            <li>Add spin $N$ to an $(N-1)$-spin system. Its Hamiltonian contribution is $H_{\text{add}}(\sigma_N) = -\sigma_N h_N$, where the field is:
              <div class="mathdisplay">$$h_N = \frac{1}{\sqrt{N}}\sum_{i=1}^{N-1} J_{i,N} \sigma_i$$</div>
            from the other spins $\sigma_i$ ($i=1,\dots,N-1$).</li>
        </ul>
        <p><strong>Cavity hypothesis:</strong> For large $N$, the field $h_N$ felt by the added spin is approximately Gaussian due to the Central Limit Theorem (sum of many weakly dependent terms). We assume we know properties of the $(N-1)$-spin system (the cavity).</p>
        <ul>
             <li>Assume the cavity system is in a state characterized by some average overlap $q$. The field $h_N$ on the new spin can be shown to be approximately $\mathcal{N}(0, \beta^2 q')$ where $q'$ is related to the cavity overlap $q$. In the high-T paramagnetic phase ($q=0$), the field is $h_N \sim \mathcal{N}(0, \beta^2)$. (Note: precise variance depends on normalization, often $\mathrm{Var}(h_N) = q$).</li>
        </ul>
        <p>The average magnetization of the added spin is then:</p>
        <div class="mathdisplay">$$ m_N = \langle \sigma_N \rangle = \mathbb{E}_{h_N}[\tanh(\beta h_N)] $$</div>

        <h4>Replica Symmetric Cavity Approach</h4>
        <p>Assume all spins behave similarly on average (replica symmetry). Let $m_i = \langle \sigma_i \rangle$ and $q = \frac{1}{N}\sum_i m_i^2$ be the Edwards-Anderson order parameter (self-overlap).</p>
        <ul>
             <li>In the high-T phase, $m_i \approx 0$ and $q \approx 0$.</li>
             <li>The field on any spin $i$, $h_i \sim \mathcal{N}(0, q)$ (using normalization where $\mathrm{Var}(J_{ij})=1/N$, giving $\mathrm{Var}(h_i) = \sum_j \mathrm{Var}(J_{ij}) \langle \sigma_j^2 \rangle \approx q$).</li>
             <li>The self-consistency equation for $m_i$ is $m_i = \mathbb{E}_{h \sim \mathcal{N}(0, q)}[\tanh(\beta h)]$. By symmetry, $m_i=0$ is always a solution.</li>
             <li>The self-consistency equation for $q$ arises from $q = \mathbb{E}[m_i^2]$:
               <div class="mathdisplay">$$ q = \mathbb{E}_{h \sim \mathcal{N}(0, q)}[\tanh^2(\beta h)] = \int_{-\infty}^{\infty} \frac{dz}{\sqrt{2\pi}} e^{-z^2/2} \tanh^2(\beta \sqrt{q}\, z) $$</div>
             This is exactly the same equation derived from the replica symmetric ansatz!</li>
        </ul>
        <p>This equation shows:</p>
        <ul>
            <li>For $\beta < 1$, only $q=0$ is stable (paramagnetic).</li>
            <li>At $\beta = \beta_c = 1$, a non-zero solution $q>0$ emerges, signaling the transition to the spin glass phase.</li>
        </ul>
        <p>The cavity method reproduces the RS results using a local, iterative perspective.</p>

        <h4>Beyond High Temperature</h4>
        <p>Below $T_c$, the assumption of a single Gaussian cavity field breaks down. There are many pure states. The cavity field distribution $P(h)$ becomes complex. The cavity method can be generalized by considering a distribution *of* distributions $P(h)$, leading to functional self-consistency related to Parisi's RSB scheme.</p>

        <p><strong>Summary (Lecture 6):</strong> The cavity method provides a physical intuition for mean-field models by analyzing the effect of adding/removing a spin. In the high-T phase, it reproduces the replica symmetric results, including the self-consistency equation for overlap $q$ and the critical temperature $T_c=1$. It naturally sets the stage for more complex analysis (like TAP equations or RSB) needed at low temperatures.</p>

        <hr>

        <h3>Lecture 7: Thouless–Anderson–Palmer (TAP) Equations and States</h3>

        <div class="overview">
        <p><strong>Overview:</strong> This lecture introduces the Thouless–Anderson–Palmer (TAP) equations, which refine naive mean-field theory for spin glasses by including the Onsager reaction term. We discuss TAP solutions as representing metastable states, their stability (Almeida–Thouless line), and the concept of complexity (number of solutions). We also mention connections to algorithms like AMP.</p>
        </div>

        <h4>Derivation of TAP Equations (Onsager Reaction Term)</h4>
        <p>Naive mean-field theory for spin $i$ might suggest $m_i = \tanh(\beta \sum_{j\neq i} J_{ij} m_j)$. This ignores the "feedback" of spin $i$ onto itself via its influence on other spins $j$. The TAP equations (1977) correct for this:</p>
        <div class="mathdisplay">$$ m_i = \tanh\Big(\beta \sum_{j\neq i} J_{ij} m_j - \beta^2 (1-q)\, m_i\Big) $$</div>
        <p>Here $q = \frac{1}{N}\sum_{j} m_j^2$ is the Edwards-Anderson order parameter for the state $\{m_j\}$. The term $-\beta^2(1-q)m_i$ is the <strong>Onsager reaction term</strong>.</p>
        <p><strong>Derivation Sketch:</strong> TAP equations arise as stationary points of the TAP free energy functional $F_{TAP}(\{m_i\})$, which can be derived via expansions (e.g., Plefka expansion) or cavity arguments.</p>
        <div class="mathdisplay">$$ F_{TAP}(\{m_i\}) \approx -\frac{\beta}{2}\sum_{i\neq j}J_{ij}m_i m_j - \sum_i S(m_i) + \frac{\beta^2}{4}(1-q)^2 N $$</div>
        <p>(Exact form varies slightly; $S(m_i)$ is the single-spin entropy term). Setting $\partial F_{TAP}/\partial m_i = 0$ yields the TAP equations.</p>

        <h4>Interpretation of the TAP Equations</h4>
        <ul>
            <li>TAP equations are $N$ coupled non-linear equations for the local magnetizations $\{m_i\}$.</li>
            <li>Each stable solution $\{m_i\}$ corresponds to a <strong>pure state</strong> (or metastable state) of the spin glass.</li>
            <li>Above $T_c$, the only solution is $m_i=0$ (paramagnetic).</li>
            <li>Below $T_c$, exponentially many solutions appear, reflecting the complex energy landscape.</li>
            <li>The Onsager term corrects the effective field, preventing divergences and ensuring consistency. It vanishes if $q=1$ (fully frozen state).</li>
        </ul>

        <h4>The Almeida–Thouless (AT) Line</h4>
        <p>Not all TAP solutions are dynamically stable. Stability is assessed by linearizing the equations or examining eigenvalues of the Hessian of $F_{TAP}$. The stability condition for the RS state (or a generic TAP state) in the SK model involves the parameter $q$.</p>
        <p>The <strong>Almeida–Thouless (AT) condition</strong> for stability of the replica symmetric solution (or a simple state) is often expressed as related to the eigenvalues of the interaction matrix. For the SK model, instability occurs when:</p>
        <div class="mathdisplay">$$ \beta^2 \mathbb{E}[(1 - \tanh^2(\beta h))^2] \ge 1 $$</div>
        <p>where $h$ is the cavity field. In zero field, this gives instability for $\beta > 1$. The AT line in the $(T, h)$ phase diagram marks where the RS solution becomes unstable to RSB. Below this line, more complex (RSB) states are needed.</p>

        <h4>Solutions and Complexity</h4>
        <p>Below $T_c$, the number of TAP solutions grows exponentially with $N$. The <strong>complexity</strong> $\Sigma(f)$ counts the density of solutions at a given free energy density $f$:</p>
        <div class="mathdisplay">$$ \mathcal{N}(f) \sim e^{N \Sigma(f)} $$</div>
        <ul>
            <li>$\Sigma(f)$ is typically positive over a range of $f$ values above the ground state free energy $f_{\min}$.</li>
            <li>$\Sigma(f_{\min}) = 0$ (ground states are rare).</li>
            <li>The existence of many metastable states (local minima of $F_{TAP}$) explains phenomena like slow dynamics and aging.</li>
            <li>The calculation of $\Sigma(f)$ itself often uses replica methods (counting solutions is related to averaging $\ln (\text{# solutions})$).</li>
        </ul>

        <h4>Rigorous Developments and Algorithms</h4>
        <ul>
            <li><strong>TAP Existence/Uniqueness:</strong> Talagrand proved uniqueness of the $m_i=0$ solution for $T>T_c$. Existence of many solutions below $T_c$ is linked to the properties of the Gibbs measure.</li>
            <li><strong>Bolthausen's Algorithm (2014):</strong> Provided an iterative scheme that converges to a TAP solution, closely related to Approximate Message Passing (AMP)<span class="citation">【22†L51-L54】</span>. His scheme dynamically incorporates the Onsager term.</li>
            <li><strong>Connection to AMP:</strong> AMP algorithms, developed for inference problems, can be seen as iterative solvers for TAP-like equations on sparse or dense graphs. The state evolution analysis of AMP rigorously tracks the dynamics of quantities like overlap $q$.</li>
        </ul>

        <p><strong>Summary (Lecture 7):</strong> TAP equations provide a self-consistent description of local magnetizations in mean-field spin glasses, including the crucial Onsager correction. Solutions correspond to pure states. The AT line signals the onset of RSB instability. The exponential number of solutions (complexity) characterizes the rugged energy landscape. TAP equations connect physics concepts to iterative algorithms like AMP.</p>
        <!-- End of Lectures 6 & 7 Content -->
    </div>

    <div id="section6" class="section">
        <!-- Repurposed section: Content from Lecture 7 focused on States/Complexity -->
        <h2>Lecture 7 cont.: TAP States, Stability, and Complexity</h2>
        <p>This section revisits key aspects from Lecture 7 concerning the nature of solutions to the TAP equations.</p>

        <h4>Interpretation of TAP Solutions as States</h4>
        <p>The Thouless–Anderson–Palmer (TAP) equations,</p>
        <div class="mathdisplay">$$ m_i = \tanh\Big(\beta \sum_{j\neq i} J_{ij} m_j - \beta^2 (1-q)\, m_i\Big), $$</div>
        <p>provide a set of self-consistent equations for the local magnetizations $m_i = \langle \sigma_i \rangle$ in a mean-field spin glass. Each distinct, stable solution vector $\{m_i\}_{i=1}^N$ is interpreted as representing a <strong>pure state</strong> of the system. These pure states correspond to the valleys or basins of attraction in the complex free energy landscape.</p>
        <ul>
            <li>In the high-temperature paramagnetic phase ($\beta < \beta_c$), there is typically only one solution: $m_i = 0$ for all $i$. This represents a single, disordered thermodynamic state.</li>
            <li>In the low-temperature spin glass phase ($\beta > \beta_c$), a multitude of non-trivial solutions emerge. This signifies the existence of many distinct thermodynamic states, separated by high free energy barriers. The system can get trapped in any of these metastable states for long times.</li>
        </ul>

        <h4>Stability and the Almeida–Thouless (AT) Line</h4>
        <p>A solution to the TAP equations must also be stable against small perturbations to be physically meaningful. The stability is analyzed by examining the Hessian matrix of the TAP free energy functional $F_{TAP}$ evaluated at the solution $\{m_i\}$. Instability is indicated by the emergence of negative eigenvalues.</p>
        <p>The <strong>Almeida–Thouless (AT) line</strong> marks the boundary in the parameter space (e.g., temperature $T$ and external field $h$) where the replica symmetric solution (or the simplest assumed state structure) becomes unstable. For the SK model in zero external field ($h=0$), the AT line coincides with the critical temperature $T_c=1$. When $h \neq 0$, the AT line occurs at a lower temperature than the freezing transition, indicating a region where a spin glass phase exists but might still be described by the RS solution before RSB becomes necessary at the AT line.</p>
        <p>The AT instability signals that the assumed symmetry (e.g., replica symmetry) is broken, and a more complex description involving replica symmetry breaking (RSB) is required to capture the true nature of the Gibbs measure.</p>

        <h4>Complexity: Counting the States</h4>
        <p>A key feature of the spin glass phase is the exponential proliferation of metastable states. The <strong>complexity</strong>, or configurational entropy, $\Sigma(f)$, quantifies the number of TAP solutions $\mathcal{N}(f)$ at a given free energy density $f$:</p>
        <div class="mathdisplay">$$ \Sigma(f) = \lim_{N\to\infty} \frac{1}{N} \ln \mathbb{E}[\mathcal{N}(f)] $$</div>
        <p>Calculations (often using the replica method themselves, applied to counting solutions) reveal the typical shape of the complexity function for the SK model:</p>
        <ul>
            <li>$\Sigma(f)$ is zero at the lowest possible free energy $f_{min}$ (corresponding to the ground state or equilibrium states).</li>
            <li>$\Sigma(f)$ increases as $f$ increases, reaching a maximum value. This indicates a vast number of higher-energy metastable states.</li>
            <li>$\Sigma(f)$ eventually decreases and becomes negative above a certain threshold free energy $f_{max}$, meaning no solutions exist at such high energies.</li>
            <li>The existence of a positive complexity $\Sigma(f) > 0$ over a range of free energies is a hallmark of the spin glass phase and is directly related to the ruggedness of the free energy landscape.</li>
        </ul>
        <p>Understanding the complexity and the distribution of these states is crucial for explaining non-equilibrium phenomena like aging and memory effects observed in real spin glasses.</p>
         <!-- End of Lecture 7 cont. Content -->
    </div>

    <div id="section7" class="section">
        <h2>Experimental Realizations of Spin Glasses (Placeholder)</h2>
        <div class="placeholder">
            <p>This section would typically discuss the physical systems where spin glass behavior is observed. Examples include:</p>
            <ul>
                <li>Dilute magnetic alloys like CuMn, AuFe.</li>
                <li>Insulating compounds.</li>
                <li>Similarities with structural glasses.</li>
            </ul>
            <p>Key experimental signatures include:</p>
            <ul>
                <li>A cusp in the AC magnetic susceptibility at the freezing temperature $T_f$.</li>
                <li>History dependence (e.g., difference between zero-field-cooled and field-cooled magnetization).</li>
                <li>Slow relaxation dynamics (aging).</li>
                <li>Absence of long-range magnetic order (e.g., via neutron scattering).</li>
            </ul>
            <p>[Detailed content on experimental methods and findings would be placed here.]</p>
        </div>
    </div>

    <div id="section8" class="section">
        <h2>Lecture 8: Applications - From TAP to Approximate Message Passing (AMP)</h2>
         <!-- Content from Markdown Lecture 8 and Perceptron part of Lecture 4 -->
        <h3>Lecture 8: From TAP to Approximate Message Passing (AMP) – Applications to Algorithms</h3>

        <div class="overview">
        <p><strong>Overview:</strong> In this final lecture, we connect theoretical developments back to algorithms. We show how TAP equations inspire iterative <strong>Approximate Message Passing (AMP)</strong> algorithms for inference and optimization. We discuss Bolthausen’s scheme for solving TAP, applications to problems like random $k$-SAT and compressed sensing, and the role of state evolution in analyzing AMP rigorously.</p>
        </div>

        <h4>From TAP Equations to AMP</h4>
        <p>Recall the TAP equations:</p>
        <div class="mathdisplay">$$ m_i = \tanh\Big(\beta \sum_{j\neq i} J_{ij} m_j - \beta^2 (1-q)\, m_i\Big). $$</div>
        <p>A natural way to solve them is via iteration. Naive iteration $m_i^{(t+1)} = \tanh(\beta \sum_{j\neq i} J_{ij} m_j^{(t)})$ often fails on dense graphs. AMP incorporates the Onsager correction dynamically. Bolthausen's iterative scheme (a form of AMP) is:</p>
        <div class="mathdisplay">$$ m_i^{(t+1)} = \tanh\Big(\beta \sum_{j\neq i} J_{ij} m_j^{(t)} - B_t \, m_i^{(t)}\Big) $$</div>
        <p>where $B_t$ is related to the Onsager term, often involving messages from the previous step, effectively $B_t \approx \beta^2 (1-q^{(t)})$, using $q^{(t)} = \frac{1}{N}\sum_j (m_j^{(t)})^2$. The precise form of AMP includes memory of the previous iteration's contribution to avoid double counting:</p>
         <div class="mathdisplay">$$ \text{Field}_{i \to j}^{(t+1)} = \sum_{k \neq i} J_{jk} \text{Message}_{k \to j}^{(t)} - \text{Correction}_t $$</div>
        <p>AMP simplifies belief propagation (BP) on dense graphs by replacing numerous messages with local estimates ($m_i$) and a global correction term (Onsager-like).</p>

        <p><strong>State Evolution:</strong> For large $N$ and random matrices $J$, the iteration's behavior can be tracked by low-dimensional "state evolution" equations. These predict the macroscopic dynamics (e.g., evolution of overlap $q^{(t)}$) and convergence properties. Rigorous analysis has confirmed that AMP achieves optimal performance in certain statistical inference problems, matching predictions from replica theory.</p>

        <h4>Applications in Optimization and Machine Learning</h4>
        <p>Spin glass ideas and algorithms inspired by them (like AMP and Survey Propagation) have found wide application:</p>
        <ul>
            <li><strong>Random $k$-Satisfiability ($k$-SAT):</strong>
                <ul>
                    <li>Physics predictions using RSB (specifically, 1RSB and survey propagation) conjectured the location of the satisfiability threshold $\alpha_{\text{sat}}(k)$ (clauses-to-variables ratio).</li>
                    <li>Survey Propagation (SP), an algorithm derived from the cavity method in the 1RSB phase, can solve large random $k$-SAT instances near the threshold where simpler algorithms fail.</li>
                    <li>The satisfiability threshold was rigorously proven for large $k$ by Ding, Sly, and Sun (2015) using methods related to the second moment method and analysis of message distributions on local tree-like structures<span class="citation">【10†L25-L30】</span>, confirming the physics predictions.</li>
                </ul>
            </li>
            <li><strong>Compressed Sensing:</strong> AMP algorithms (Donoho, Maleki, Montanari, 2009) are used to reconstruct sparse signals from underdetermined linear measurements. State evolution precisely characterizes their performance, matching theoretical limits derived from replica theory<span class="citation">【9】</span>.</li>
            <li><strong>Error-Correcting Codes:</strong> Belief propagation (related to the cavity method) is used for decoding LDPC codes. Spin glass concepts help understand phase transitions in decoding performance.</li>
            <li><strong>Neural Networks (Perceptron Capacity):</strong> As mentioned in Lecture 4, Gardner (1988) used the replica method to calculate the maximum number of random patterns ($\alpha_c = P/N$) a simple perceptron can store<span class="citation">【10】</span>. This involved analyzing the volume of the space of weight vectors satisfying the constraints, a problem amenable to statistical physics methods. The calculation often required RS or 1RSB assumptions, depending on the specific constraints (e.g., binary vs. spherical weights).</li>
            <li><strong>Community Detection, Matrix Factorization:</strong> AMP and related message-passing algorithms are applied to graph partitioning and estimating low-rank structures in noisy data, again often achieving theoretically optimal performance predicted by statistical physics.</li>
        </ul>

        <h4>Summary and Outlook</h4>
        <p>This lecture series has journeyed from the basic concepts of spin glasses (disorder, frustration) through key models (REM, SK) and powerful analytical techniques (replicas, cavity, TAP, interpolation). We saw how the need to describe the complex low-temperature phase led to the groundbreaking concept of Replica Symmetry Breaking.</p>
        <p>Key takeaways:</p>
        <ul>
            <li>Spin glasses are prototypes of complex systems with rugged energy landscapes and many metastable states.</li>
            <li>Methods developed for spin glasses (replica trick, cavity method, TAP, RSB) provide a powerful, though sometimes non-rigorous, framework for analyzing disordered systems.</li>
            <li>Rigorous techniques (Gaussian comparison, interpolation) have validated core physics predictions, notably the Parisi solution for the SK model.</li>
            <li>These ideas have profound implications beyond physics, inspiring state-of-the-art algorithms (AMP, SP) for challenging problems in computer science, information theory, and machine learning.</li>
            <li>The interplay between statistical physics, probability theory, and computer science continues to be a fertile ground for research.</li>
        </ul>
        <p>This concludes the lecture series on Spin Glass Theory.</p>
        <hr>
        <!-- End of Lecture 8 Content -->
    </div>

    <div id="section9" class="section">
        <h2>Current Research Directions (Placeholder)</h2>
        <div class="placeholder">
            <p>Spin glass theory remains an active area of research. Some current directions include:</p>
            <ul>
                <li><strong>Finite-dimensional systems:</strong> Understanding spin glasses on realistic lattices (e.g., 3D) beyond mean-field approximations. The nature of the spin glass phase in finite dimensions (e.g., droplet theory vs. RSB picture) is still debated.</li>
                <li><strong>Quantum spin glasses:</strong> Incorporating quantum fluctuations (e.g., transverse fields) leads to quantum annealing and new phase transitions.</li>
                <li><strong>Dynamics:</strong> Developing a complete theory for the slow dynamics, aging, and memory effects observed experimentally and numerically.</li>
                <li><strong>Beyond pairwise interactions:</strong> Studying models with multi-spin interactions (p-spin models) relevant to constraint satisfaction problems and structural glasses.</li>
                <li><strong>Algorithmic thresholds:</strong> Understanding the gap between information-theoretic limits (when solutions exist) and algorithmic limits (when efficient algorithms can find them) in computational problems related to spin glasses.</li>
                <li><strong>Deep learning connections:</strong> Exploring links between the energy landscapes of spin glasses and the loss landscapes of deep neural networks.</li>
                <li><strong>Mathematical rigor:</strong> Continuing efforts to provide rigorous proofs for predictions made using heuristic methods like the replica trick and cavity method, especially for more complex models or finite dimensions.</li>
            </ul>
            <p>[More specific research topics and recent advancements would be added here.]</p>
        </div>
    </div>

    <div id="section10" class="section">
        <h2>References and Further Reading</h2>
        <!-- Bibliography from Markdown -->
        <ul class="reference-list">
             <li>[1] Derrida, B. (1980). <em>Random-energy model: An exactly solvable model of disordered systems</em>. Physical Review B, 24(5), 2613–2626.</li>
             <li>[2] Sherrington, D., & Kirkpatrick, S. (1975). <em>Solvable model of a spin-glass</em>. Physical Review Letters, 35(26), 1792–1796.</li>
             <li>[3] Parisi, G. (1979). <em>Infinite number of order parameters for spin-glasses</em>. Physical Review Letters, 43(23), 1754–1756. (And subsequent papers in J. Phys. A, 1980).</li>
             <li>[4] Guerra, F. (2003). <em>Broken replica symmetry bounds in the mean field spin glass model</em>. Communications in Mathematical Physics, 233(1), 1–12.</li>
             <li>[5] Talagrand, M. (2006). <em>The Parisi formula</em>. Annals of Mathematics, 163(1), 221–263. Also see his books: <em>Spin Glasses: A Challenge for Mathematicians</em> (2003) and <em>Mean Field Models for Spin Glasses</em> (Vol. I & II, 2010).</li>
             <li>[6] Bolthausen, E. (2014). <em>An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model</em>. Communications in Mathematical Physics, 325(1), 333–366.</li>
             <li>[7] Mézard, M., Parisi, G., & Virasoro, M. A. (1987). <em>Spin Glass Theory and Beyond</em>. World Scientific Lecture Notes in Physics, Vol. 9. World Scientific.</li>
             <li>[8] Ding, J., Sly, A., & Sun, N. (2015). <em>Proof of the satisfiability conjecture for large k</em>. Proceedings of the 47th Annual ACM Symposium on Theory of Computing (STOC '15), 59–68.</li>
             <li>[9] Donoho, D. L., Maleki, A., & Montanari, A. (2009). <em>Message-passing algorithms for compressed sensing</em>. Proceedings of the National Academy of Sciences (PNAS), 106(45), 18914–18919.</li>
             <li>[10] Gardner, E. (1988). <em>The space of interactions in neural network models</em>. Journal of Physics A: Mathematical and General, 21(1), 257–270.</li>
             <li>Further reading: Nishimori, H. (2001). <em>Statistical Physics of Spin Glasses and Information Processing: An Introduction</em>. Oxford University Press.</li>
             <li>Further reading: Castellani, T., & Cavagna, A. (2005). <em>Spin-glass theory for pedestrians</em>. Journal of Statistical Mechanics: Theory and Experiment, P05012.</li>
        </ul>
        <!-- End of Bibliography -->
        <hr>
    </div>
</body>
</html>